{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46b0a720",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "195b2d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party imports\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import buteo as beo\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Local imports\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from utils.constants import DATA_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2aa2382",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "\n",
    "The model will take as input an image containing the 9 spectral bands from Sentinel-2 (normalised and split up into patches of 32x32). The output will be the building density of the pixel ranging between 0-100, which means we are doing a regression task as we are trying to predict specific values.\n",
    "\n",
    "In this architecture, there is an encoder block where the number of channels is increased to 128. Because of the padding, the resolution of the output channels is unchanged. In the decoder block, there are extra convolutions that bring the number of channels back to a single one that contains the building density.\n",
    "\n",
    "To help the model, we force the values to lie between the range of 0-100, since any other values will never be correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b437945",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvNet(nn.Module):\n",
    "  \"\"\"\n",
    "  A simple convolutional neural network that encodes input images\n",
    "  and decodes them back to a single-channel output image.\n",
    "  \"\"\"\n",
    "  def __init__(self, input_channels: int, output_min: float, output_max: float) -> None:\n",
    "    super(SimpleConvNet, self).__init__()\n",
    "    self.output_min = output_min\n",
    "    self.output_max = output_max\n",
    "\n",
    "    # An encoder without a bottleneck\n",
    "    self.encoder = nn.Sequential(\n",
    "      nn.Conv2d(input_channels, 64, kernel_size=3, padding=1),\n",
    "      nn.ReLU(inplace=True),\n",
    "      nn.BatchNorm2d(64),\n",
    "      nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "      nn.ReLU(inplace=True),\n",
    "      nn.BatchNorm2d(128),\n",
    "    )\n",
    "\n",
    "    # Simple decoder\n",
    "    self.decoder = nn.Sequential(\n",
    "      nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "      nn.ReLU(inplace=True),\n",
    "      nn.BatchNorm2d(64),\n",
    "      nn.Conv2d(64, 1, kernel_size=3, padding=1),\n",
    "    )\n",
    "  \n",
    "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Forward pass through the network.\n",
    "    \"\"\"\n",
    "    x = self.encoder(x)\n",
    "    x = self.decoder(x)\n",
    "\n",
    "\n",
    "    # Clamp the output values to be within [output_min, output_max]\n",
    "    x = torch.clamp(x, self.output_min, self.output_max)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc17bc9",
   "metadata": {},
   "source": [
    "## Initalise the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fdd9333",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_channels = 9 # Sentinel 2 initially.\n",
    "\n",
    "# Initialise the model\n",
    "model = SimpleConvNet(input_channels, 0.0, 100.0) # Since we know the labels will always be [0.0, 100.0]\n",
    "\n",
    "# Constants for the model\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "177812d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyDataset(Dataset):\n",
    "  \"\"\"\n",
    "  A simple dataset class that loads numpy arrays.\n",
    "  \"\"\"\n",
    "  def __init__(self, x_train: np.ndarray, y_train: np.ndarray, data_is_channel_last: bool = False) -> None:\n",
    "    if data_is_channel_last:\n",
    "      x_train = beo.channel_last_to_first(x_train)\n",
    "      y_train = beo.channel_last_to_first(y_train)\n",
    "    \n",
    "    self.x_train = torch.from_numpy(x_train).float()\n",
    "    self.y_train = torch.from_numpy(y_train).float()\n",
    "  \n",
    "  def __len__(self) -> int:\n",
    "    \"\"\"\n",
    "    Returns the number of samples in the dataset.\n",
    "    \"\"\"\n",
    "    return len(self.x_train)\n",
    "  \n",
    "  def __getitem__(self, idx: int) -> tuple:\n",
    "    \"\"\"\n",
    "    Returns a single sample from the dataset.\n",
    "    \"\"\"\n",
    "    x = self.x_train[idx]\n",
    "    y = self.y_train[idx]\n",
    "    \n",
    "    return x, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3049028",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c84a79ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load(os.path.join(DATA_FOLDER, 'train.npz'))['x_s2'] # Initially we only load the S2 data\n",
    "y_train = np.load(os.path.join(DATA_FOLDER, 'train.npz'))['y']\n",
    "\n",
    "# Prepare the data for pytorch\n",
    "def callback(x: np.ndarray, y: np.ndarray) -> tuple:\n",
    "    \"\"\"\n",
    "    Callback function to prepare the data for PyTorch.\n",
    "    \"\"\"\n",
    "    x = torch.from_numpy(x).float()\n",
    "    y = torch.from_numpy(y).float()\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13416849",
   "metadata": {},
   "source": [
    "### Create Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705f9fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = NumpyDataset(x_train, y_train, data_is_channel_last=True)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec77ca24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.backends.mps.is_available())  # Check if MPS is available\n",
    "print(torch.backends.mps.is_built())      # Check if PyTorch was built with MPS support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6e6ede",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6303148a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]: 100%|████████████████████████████████████████████████| 1966/1966 [05:56<00:00,  5.52it/s, loss=184.975243]\n",
      "Epoch [2/10]: 100%|████████████████████████████████████████████████| 1966/1966 [05:50<00:00,  5.62it/s, loss=157.885668]\n",
      "Epoch [3/10]: 100%|████████████████████████████████████████████████| 1966/1966 [05:41<00:00,  5.76it/s, loss=146.522431]\n",
      "Epoch [4/10]: 100%|████████████████████████████████████████████████| 1966/1966 [04:55<00:00,  6.65it/s, loss=137.826457]\n",
      "Epoch [5/10]: 100%|████████████████████████████████████████████████| 1966/1966 [05:00<00:00,  6.55it/s, loss=133.220347]\n",
      "Epoch [6/10]: 100%|████████████████████████████████████████████████| 1966/1966 [05:17<00:00,  6.20it/s, loss=128.514122]\n",
      "Epoch [7/10]: 100%|████████████████████████████████████████████████| 1966/1966 [05:14<00:00,  6.24it/s, loss=126.024043]\n",
      "Epoch [8/10]: 100%|████████████████████████████████████████████████| 1966/1966 [05:17<00:00,  6.18it/s, loss=123.199071]\n",
      "Epoch [9/10]: 100%|████████████████████████████████████████████████| 1966/1966 [05:26<00:00,  6.02it/s, loss=121.002593]\n",
      "Epoch [10/10]: 100%|███████████████████████████████████████████████| 1966/1966 [05:17<00:00,  6.19it/s, loss=119.491688]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimiser = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "  running_loss = 0.0\n",
    "\n",
    "  # Initialise the progress bar for training\n",
    "  train_pbar = tqdm(dataLoader, total=len(dataLoader), ncols=120)\n",
    "\n",
    "  for i, (inputs, targets) in enumerate(train_pbar):\n",
    "    # Move inputs and targets to the device\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "    # Zero the gradients\n",
    "    optimiser.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "    # Print the statistics\n",
    "    current_loss = loss.item()\n",
    "    running_loss += current_loss\n",
    "    mean_loss = running_loss / (i + 1)\n",
    "\n",
    "    # Update the progress bar\n",
    "    train_pbar.set_description(f\"Epoch [{epoch + 1}/{EPOCHS}]\")\n",
    "    print_dict = { 'loss': f'{mean_loss:4f}'}\n",
    "    train_pbar.set_postfix(print_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d846acb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model \n",
    "model_folder = '../models'\n",
    "\n",
    "# Ensure the output directory exists\n",
    "Path(model_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "torch.save(model.state_dict(), os.path.join(model_folder, 'model_01.pth'))\n",
    "del dataset, dataloader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flood-monitoring",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
