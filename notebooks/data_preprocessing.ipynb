{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7608853",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42e3ec6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Third party imports\n",
    "import numpy as np\n",
    "import buteo as beo\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import import_ipynb\n",
    "\n",
    "# Local imports\n",
    "from utilities import custom_subplots\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c96604d",
   "metadata": {},
   "source": [
    "## Split Train and Test Set\n",
    "\n",
    "To make sure the test set is completely independant from the train set, we will split the data before making the patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9030a205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 78/78 [00:18<00:00,  4.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Labels:  (28120, 32, 32, 1) (28120, 32, 32, 2) (28120, 32, 32, 9) (28120, 32, 32, 4)\n",
      "Test Indices:  [68 39 44  7  1 17 41 56]\n",
      "Testing Labels:  (1392, 32, 32, 1) (1392, 32, 32, 2) (1392, 32, 32, 9) (1392, 32, 32, 4)\n",
      "Validation Indices:  [15  6 22 57 45 22 31]\n",
      "Validation Labels:  (3292, 32, 32, 1) (3292, 32, 32, 2) (3292, 32, 32, 9) (3292, 32, 32, 4)\n"
     ]
    }
   ],
   "source": [
    "# Set the data folder\n",
    "DATA_FOLDER = os.path.abspath(os.path.join(os.getcwd(), '../data'))\n",
    "\n",
    "# Set the seeds\n",
    "np.random.seed(17)\n",
    "TEST_INDICES = np.random.choice(78, int(78 * 0.2))\n",
    "VAL_INDICES = TEST_INDICES[:len(TEST_INDICES) // 2]\n",
    "TEST_INDICES = TEST_INDICES[len(TEST_INDICES) // 2:]\n",
    "\n",
    "# We will use patch size of 32x32 to reduce the amount of RAM required to run the model\n",
    "PATCH_SIZE = 32\n",
    "\n",
    "# With zero offsets, every pixel in the training data would only be sampled once.\n",
    "# By setting overlaps, we can sample pixel multiple times but in different places in the patches.\n",
    "N_OFFSETS = 3 # ((0, 0), (8, 8), (16, 16), (24, 24))\n",
    "\n",
    "# Patch lists\n",
    "training_labels = []\n",
    "training_s1 = []\n",
    "training_s2 = []\n",
    "training_dem = []\n",
    "\n",
    "val_labels = []\n",
    "val_s1 = []\n",
    "val_s2 = []\n",
    "val_dem = []\n",
    "\n",
    "testing_labels = []\n",
    "testing_s1 = []\n",
    "testing_s2 = []\n",
    "testing_dem = []\n",
    "\n",
    "labels_paths = sorted(glob(os.path.join(DATA_FOLDER, 'CCAI_FLOODS_DATA/label_*.tif')))\n",
    "s1_paths = sorted(glob(os.path.join(DATA_FOLDER, 'CCAI_FLOODS_DATA/s1_*.tif')))\n",
    "s2_paths = sorted(glob(os.path.join(DATA_FOLDER, 'CCAI_FLOODS_DATA/s2_*.tif')))\n",
    "dem_paths = sorted(glob(os.path.join(DATA_FOLDER, 'CCAI_FLOODS_DATA/dem_*.tif')))\n",
    "\n",
    "# Read and order the tiles in a temporary folder\n",
    "for image in tqdm(\n",
    "  zip(labels_paths, s1_paths, s2_paths, dem_paths),\n",
    "  total=len(labels_paths),\n",
    "  ncols=120\n",
    "):\n",
    "  label_path, s1_path, s2_path, dem_path = image\n",
    "\n",
    "  # Get the name and number of the patches\n",
    "  label_name = os.path.splitext(os.path.basename(label_path))[0]\n",
    "  img_idx = int(label_name.split('_')[1])\n",
    "\n",
    "  # Get the data from the tiles\n",
    "  arr_label = beo.raster_to_array(label_path, filled=True, fill_value=0.0)\n",
    "\n",
    "  # Handle any potential errors\n",
    "  np.clip(arr_label, 0.0, 100.0, out=arr_label)\n",
    "  arr_label[np.isnan(arr_label)] = 0.0\n",
    "\n",
    "  # Read the tiles\n",
    "  arr_s1 = beo.raster_to_array(s1_path, filled=True, fill_value=0.0)\n",
    "  arr_s2 = beo.raster_to_array(s2_path, filled=True, fill_value=0.0)\n",
    "  arr_dem = beo.raster_to_array(dem_path, filled=True, fill_value=0.0)\n",
    "  \n",
    "  # Generate the patches\n",
    "  label_patches = beo.array_to_patches(arr_label, PATCH_SIZE, n_offsets=N_OFFSETS)\n",
    "  s1_patches = beo.array_to_patches(arr_s1, PATCH_SIZE, n_offsets=N_OFFSETS)\n",
    "  s2_patches = beo.array_to_patches(arr_s2, PATCH_SIZE, n_offsets=N_OFFSETS)\n",
    "  dem_patches = beo.array_to_patches(arr_dem, PATCH_SIZE, n_offsets=N_OFFSETS)\n",
    "\n",
    "  # Sanity check to ensure that the right images were chosen\n",
    "  assert label_patches.shape[0:3] == s1_patches.shape[0:3] == s2_patches.shape[0:3] == dem_patches.shape[0:3], \\\n",
    "    f'Patch shapes do not match for image {img_idx}'\n",
    "  \n",
    "  if img_idx in TEST_INDICES:\n",
    "    testing_labels.append(label_patches)\n",
    "    testing_s1.append(s1_patches)\n",
    "    testing_s2.append(s2_patches)\n",
    "    testing_dem.append(dem_patches)\n",
    "  elif img_idx in VAL_INDICES:\n",
    "    val_labels.append(label_patches)\n",
    "    val_s1.append(s1_patches)\n",
    "    val_s2.append(s2_patches)\n",
    "    val_dem.append(dem_patches)\n",
    "  else:\n",
    "    training_labels.append(label_patches)\n",
    "    training_s1.append(s1_patches)\n",
    "    training_s2.append(s2_patches)\n",
    "    training_dem.append(dem_patches)\n",
    "\n",
    "# Merge the patches back together\n",
    "training_labels = np.concatenate(training_labels, axis=0)\n",
    "training_s1 = np.concatenate(training_s1, axis=0)\n",
    "training_s2 = np.concatenate(training_s2, axis=0)\n",
    "training_dem = np.concatenate(training_dem, axis=0)\n",
    "\n",
    "val_labels = np.concatenate(val_labels, axis=0)\n",
    "val_s1 = np.concatenate(val_s1, axis=0)\n",
    "val_s2 = np.concatenate(val_s2, axis=0)\n",
    "val_dem = np.concatenate(val_dem, axis=0)\n",
    "\n",
    "testing_labels = np.concatenate(testing_labels, axis=0)\n",
    "testing_s1 = np.concatenate(testing_s1, axis=0)\n",
    "testing_s2 = np.concatenate(testing_s2, axis=0)\n",
    "testing_dem = np.concatenate(testing_dem, axis=0)\n",
    "\n",
    "print('Training Labels: ', training_labels.shape, training_s1.shape, training_s2.shape, training_dem.shape)\n",
    "print('Test Indices: ', TEST_INDICES)\n",
    "print('Testing Labels: ', testing_labels.shape, testing_s1.shape, testing_s2.shape, testing_dem.shape)\n",
    "print('Validation Indices: ', VAL_INDICES)\n",
    "print('Validation Labels: ', val_labels.shape, val_s1.shape, val_s2.shape, val_dem.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2ae0c7",
   "metadata": {},
   "source": [
    "## Normalisation\n",
    "\n",
    "**Sentinel-1**: The current data is expressed in dB. The data is first clipped to the range (-5 dB, 35 dB). Afterwards each value is normalised to (0, 1) by subtracting the mean and dividing by the standard deviation.\n",
    "\n",
    "**Sentinel-2**: The Sentinel-2 images are uint16 with values ranging from 0 to 65,535 which are called digital numbers (DN). The physical value derived from this is the reflectance defined as DN/10,000. The reflectance is usually between 0-1 but can be higher due to surface or cloud effects. By convention, Sentinel-2 data is clipped to the range (0, 10,000). Afterwards it is normalised to (0, 1) as well.\n",
    "\n",
    "**CopDEM**: The channels in the DEM are already normalised. The elevation (4th channel) is normalised with respect to the highest point on the Earth, Mt. Everest. The slope (3rd band) is also normalised with respect to a slope of 90&deg;. The first two bands contain the orientation of the slope. The orientation is a cyclical variable (0&deg;-360&deg;), so we need two channels to encode and normalise this information. This can be done by storing the sine of the orientation in the first band and the cosine in the second band to keep the range between (0, 1). The final result is:\n",
    "\n",
    "$channel 1 = [sin(orientation) + 1]/2$\n",
    "\n",
    "$channel 2 = [cos(orientation) + 1]/2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a8939c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_train, _statdict = beo.scaler_truncate(training_s1, -35.0, 5.0)\n",
    "val_train, _statdict = beo.scaler_truncate(val_s1, -35.0, 5.0)\n",
    "s1_test, _statdict = beo.scaler_truncate(testing_s1, -35.0, 5.0)\n",
    "\n",
    "s2_train, _statdict = beo.scaler_truncate(training_s2, 0.0, 10000.0)\n",
    "val_s2, _statdict = beo.scaler_truncate(val_s2, 0.0, 10000.0)\n",
    "s2_test, _statdict = beo.scaler_truncate(testing_s2, 0.0, 10000.0)\n",
    "\n",
    "# If labels are not already float32, convert them to float32\n",
    "train_labels = training_labels.astype(np.float32, copy=False)\n",
    "val_labels = val_labels.astype(np.float32, copy=False)\n",
    "test_labels = testing_labels.astype(np.float32, copy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ce1f24",
   "metadata": {},
   "source": [
    "Let's save all our work to disk as NumPy arrays, so that at any moment in time, we could reload the data again and use it to train the model. When loading the data again, the data can be kept on disk, and only necessary bits can be loaded into memory upon access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8086fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(os.path.join(DATA_FOLDER, 'train.npz'), x_s1=s1_train, x_s2=s2_train, x_dem=training_dem, y=train_labels)\n",
    "np.savez_compressed(os.path.join(DATA_FOLDER, 'val.npz'), x_s1=val_s1, x_s2=val_s2, x_dem=val_dem, y=val_labels)\n",
    "np.savez_compressed(os.path.join(DATA_FOLDER, 'test.npz'), x_s1=s1_test, x_s2=s2_test, x_dem=testing_dem, y=test_labels)\n",
    "\n",
    "# Clean memory\n",
    "del s1_train, s2_train, training_dem, train_labels\n",
    "del val_s1, val_s2, val_dem, val_labels\n",
    "\n",
    "del s1_test, s2_test, testing_dem, test_labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flood-monitoring",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
